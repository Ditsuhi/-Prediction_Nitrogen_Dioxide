{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiConvLSTM_NO2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ditsuhi/Nitrogen_Dioxide_Prediction/blob/main/BiConvLSTM_NO2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0W6bHf7DQV"
      },
      "source": [
        "# import all required libraries\n",
        "\n",
        "import zipfile\n",
        "from glob import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.interpolate import NearestNDInterpolator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import ConvLSTM2D, BatchNormalization\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers.convolutional import  Conv2D \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDP5utWzGuOB"
      },
      "source": [
        "# To calculate nearest neighbor interpolation for meteorological data\n",
        "\n",
        "def CalcNNvalue(array_interpolate):\n",
        "  \n",
        "  array_float = array_interpolate.astype(float)\n",
        "  knowncell_position= np.argwhere(array_float!=0)  \n",
        "  knowncell_value = array_float[array_float!=0] \n",
        "  unknowncell_position = np.argwhere(array_float==0)\n",
        "  myInterpolator = NearestNDInterpolator(knowncell_position, knowncell_value) \n",
        "  unknown_values = myInterpolator(unknowncell_position)\n",
        "  array_float[array_float == 0 ] = unknown_values\n",
        "  return array_float.tolist()\n",
        "\n",
        "\n",
        "def calc_NN_fullData(full_data):\n",
        "  NN_list =[]\n",
        "  for item in full_data:    \n",
        "    try: \n",
        "      NN_list.append(CalcNNvalue(item))\n",
        "    except IndexError:\n",
        "      NN_list.append(item.tolist())  \n",
        "  return NN_list\n",
        "\n",
        "\n",
        "def calculate_NN_fullData_allAttributes (df_all):\n",
        "  df_all_NN_list = []\n",
        "  # The number in the range is the number of meteorological features \n",
        "  # to be interpolated using nearest neighbor interpolation.  \n",
        "  for attr_numb in range(7): \n",
        "    certain_attr = df_all[:, :, attr_numb]    \n",
        "    certain_attr_reshaped= certain_attr.reshape(certain_attr.shape[0], 20, 17)\n",
        "    certain_attr_reshaped_NN = calc_NN_fullData(certain_attr_reshaped) \n",
        "    certain_attr_reshaped_NN_original_shape = np.reshape(certain_attr_reshaped_NN, (certain_attr.shape[0], 340))\n",
        "    df_all_NN_list.append(certain_attr_reshaped_NN_original_shape.tolist())   \n",
        "  df_all_NN_array = np.dstack((item)for item in df_all_NN_list)  \n",
        "  return df_all_NN_array\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVuqLVTRbKkn"
      },
      "source": [
        "#unzip data giving the path of certain dataset\n",
        "\n",
        "path = '/content/AirMetTrafMadridDataCSV_2019.zip'\n",
        "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "airMetTraf = glob(\"/content/*.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ctm4qcddU9",
        "outputId": "4ede97f6-0bed-473e-ec29-c5f31976240e"
      },
      "source": [
        "#sort dataset with chronological order\n",
        "\n",
        "def sortingFiles(eachFile):\n",
        "    return int(eachFile) if eachFile.isdigit() else eachFile\n",
        "def natural_keys(eachFile):\n",
        "    return [sortingFiles(c) for c in re.split('(\\d+)',eachFile)]\n",
        "\n",
        "sorted_airMetTraf= sorted(airMetTraf, key = natural_keys)\n",
        "sorted_airMetTraf_2019 = sorted_airMetTraf[:4344]\n",
        "#sorted_airMetTraf_2020 = sorted_airMetTraf[4344:]\n",
        "\n",
        "\n",
        "# These are the feyures from the matrices: FID\t NO2\t UV\t windSpeed\t windDir\t Temp\t Humidity\t Pressure\t SolarRad\t Prec\t intensidad\t ocupacion\t carga\t vmed\n",
        "\n",
        "df = [pd.read_csv(f, usecols=[' NO2', ' UV',  ' windSpeed', ' windDir', ' Temp', ' Humidity', ' Pressure', ' SolarRad', ' Prec', ' intensidad',\t' ocupacion',\t' carga',\t ' vmed']).values for f in sorted_airMetTraf_2019]\n",
        "\n",
        "\n",
        "df_all  = np.asarray(df)\n",
        "\n",
        "# This step is for outlier handling (Temperature:res; Humidity:reshum;\n",
        "# and Average Speed:speed)\n",
        "res = np.where(df_all[:, :, 4] < -3)\n",
        "reshum = np.where(df_all[:, :, 5] < 0)\n",
        "speed = np.where(df_all[:, :, 12] < 0)\n",
        "\n",
        "print(len(res[1]))\n",
        "print(len(reshum[1]))\n",
        "print(len(speed[0]))\n",
        "\n",
        "# all values for a temperature data below -3 are converted to an average\n",
        "# before and after the values.\n",
        "\n",
        "for i in range(len(res[0])):\n",
        "  if df_all[:, :, 4][res[0][i]][res[1][i]-1] > -3 and df_all[:, :, 4][res[0][i]][res[1][i]+1] > -3:\n",
        "    df_all[:, :, 4][res[0][i]][res[1][i]] = (df_all[:, :, 4][res[0][i]][res[1][i]-1]+df_all[:, :, 4][res[0][i]][res[1][i]+1])/2\n",
        "\n",
        "\n",
        "# all values for a humidity data below 0 are converted to an average\n",
        "# before and after the values.\n",
        "\n",
        "for i in range(len(reshum[0])):\n",
        "  if df_all[:, :, 5][reshum[0][i]][reshum[1][i]-1] >= 0 and df_all[:, :, 5][reshum[0][i]][reshum[1][i]+1] >= 0:\n",
        "    df_all[:, :, 5][reshum[0][i]][reshum[1][i]] = (df_all[:, :, 5][reshum[0][i]][reshum[1][i]-1]+df_all[:, :, 5][reshum[0][i]][reshum[1][i]+1])/2\n",
        "\n",
        "\n",
        "# all values for a speed data below 0 are converted to 0.\n",
        "\n",
        "for i in range(len(speed[0])):\n",
        "  df_all[:, :, 12][speed[0][i]][speed[1][i]] = 0\n",
        "\n",
        "\n",
        "# deleting precipitation as the majority of the values are equal to 0.\n",
        "\n",
        "df_all_non_prec = np.delete(df_all, 8, 2)\n",
        "air= df_all_non_prec[:, :, 0].reshape(-1, 340, 1)\n",
        "traf =  df_all_non_prec[:, :, 8:12].reshape(-1, 340, 4)\n",
        "NN_dataframe = calculate_NN_fullData_allAttributes (df_all_non_prec[:, :, 1:8])\n",
        "df_air_NN_Met = np.concatenate((air, idw_dataframe, traf), axis=2)\n",
        "not_nun = np.nan_to_num(df_air_NN_Met)\n",
        "round_data = np.round(not_nun, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "820\n",
            "3\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-mXHyL6Bdbe",
        "outputId": "94a26182-d5c2-4c5d-b41d-50f51db0a91e"
      },
      "source": [
        "round_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4344, 340, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqmvg5okqAT6"
      },
      "source": [
        "# split dataset to X and y (dependent and independent)\n",
        "\n",
        "def split_sequence(sequence, time_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "   \n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + 12\n",
        "    \n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix+time_steps > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern    \n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix+time_steps]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn np.array(X), np.array(y)\n",
        " \n",
        "\n",
        "# define input sequence\n",
        "raw_seq = round_data\n",
        "# choose a number of time steps (there are two case of time lags: 6-hour and 12-hour)\n",
        "time_steps = 6\n",
        "X, y = split_sequence(raw_seq, time_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3GM3QIzFQYE",
        "outputId": "85daa15d-3a6b-45e4-e9e6-4071e470b222"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4326, 340, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXKYD1ZQqAXp"
      },
      "source": [
        "#split data to train and test sets\n",
        "\n",
        "X_train_notNorm, X_test_notNorm, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xddYHLEUFWpu",
        "outputId": "112e4c87-ea39-4e26-a02b-3e213e305d45"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(866, 340, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbCaxh3OtBGA"
      },
      "source": [
        "# to normalise train data using MinMaxScaler\n",
        "number_selected_columns = 12\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), copy = False)\n",
        "train_Normalised = X_train_notNorm.reshape(-1, 340*number_selected_columns)\n",
        "test_Normalised = X_test_notNorm.reshape(-1, 340*number_selected_columns)\n",
        "\n",
        "train_scaled = scaler.fit_transform(train_Normalised)\n",
        "test_scaled = scaler.transform(test_Normalised)\n",
        "\n",
        "X_train = train_scaled.reshape(X_train_notNorm.shape[0], X_train_notNorm.shape[1], X_train_notNorm.shape[2], X_train_notNorm.shape[3])\n",
        "X_test = test_scaled.reshape(X_test_notNorm.shape[0], X_test_notNorm.shape[1], X_test_notNorm.shape[2], X_test_notNorm.shape[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p1ln_Xs7KiN"
      },
      "source": [
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_train_reshaped = y_train.reshape((y_train.shape[0], 20, 17*number_selected_columns, 1))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 20, 17*number_selected_columns, 1))\n",
        "y_test_reshaped = y_test.reshape(y_test.shape[0], 20, 17*number_selected_columns, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k6FiLdbEH3q",
        "outputId": "61fd4298-4231-409e-ae20-145f528bd008"
      },
      "source": [
        "X_train_reshaped.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3460, 12, 20, 204, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rJcmnutHFgD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyax0t7jHFi1"
      },
      "source": [
        "# In the following two cells are the code for parameter optimisation.\n",
        "# First of all, the class was created in order to split data.\n",
        "\n",
        "class BlockingTimeSeriesSplit():\n",
        "    def __init__(self, n_splits):\n",
        "        self.n_splits = n_splits\n",
        "    \n",
        "    def get_n_splits(self, X, y, groups):\n",
        "        return self.n_splits\n",
        "    \n",
        "    def split(self, X, y=None, groups=None):\n",
        "        n_samples = len(X)\n",
        "        k_fold_size = n_samples // self.n_splits\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        margin = 0\n",
        "        for i in range(self.n_splits):\n",
        "            start = i * k_fold_size\n",
        "            stop = start + k_fold_size\n",
        "            mid = int(0.8 * (stop - start)) + start\n",
        "            yield indices[start: mid], indices[mid + margin: stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMS0FPdHHFlS"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "btscv = BlockingTimeSeriesSplit(n_splits=3)\n",
        "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
        "\n",
        "\n",
        "#define the grid search parameters\n",
        "\n",
        "\n",
        "optimizer = ['RMSprop',  'Adam']\n",
        "kernel_size = [(3, 3), (5, 5), (7, 7), (9, 9), (11, 11)]\n",
        "filters= [8, 16]\n",
        "merge_mode=['sum', 'concat', 'ave']\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
        "param_grid = dict(filters=filters,  kernel_size=kernel_size, optimizer=optimizer, merge_mode= merge_mode)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=btscv)\n",
        "grid_result = grid.fit(X_train_reshaped, y_train_reshaped, epochs = 20)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Q17nvrHLyE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG0zCR9_JBt6"
      },
      "source": [
        "# define the architecture of the proposed model\n",
        "\n",
        "def create_model(number_selected_columns=12, optimizer='adam', kernel_size=(9, 9), filters=8, merge_mode=\"concat\"):\n",
        "    \n",
        "    model = Sequential()    \n",
        "    model.add(Bidirectional(ConvLSTM2D(input_shape=(None, 20, 17*number_selected_columns, 1),  filters=filters,  kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))\n",
        "    \n",
        "    model.add(BatchNormalization())  \n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters, kernel_size=kernel_size, padding=\"same\", return_sequences=True), merge_mode=merge_mode))    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Bidirectional(ConvLSTM2D(filters=filters,  kernel_size=kernel_size, padding=\"same\"), merge_mode=merge_mode))     \n",
        "    model.add(BatchNormalization())           \n",
        "    model.add(Conv2D(filters=1, kernel_size=(1, 1),\n",
        "                activation='relu',\n",
        "                padding='same', data_format='channels_last'))\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.build(input_shape=(None, 12, 20, 17*number_selected_columns, 1))    \n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZe5LPkN65JR",
        "outputId": "9f7692c7-930a-46ff-bfe2-eceb82353c78"
      },
      "source": [
        "mod = create_model(number_selected_columns=12, optimizer='adam', kernel_size=(9, 9), filters=8, merge_mode=\"concat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection (None, 12, 20, 204, 16)   46720     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 20, 204, 16)   64        \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 12, 20, 204, 16)   124480    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 12, 20, 204, 16)   64        \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 20, 204, 16)       124480    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 20, 204, 16)       64        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 204, 1)        17        \n",
            "=================================================================\n",
            "Total params: 295,889\n",
            "Trainable params: 295,793\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvVtdK0EGOME"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=2)\n",
        "mod.fit(X_train_reshaped, y_train_reshaped, batch_size = 8, epochs=100, verbose=2, validation_split=0.25, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG7DPrZKGQub"
      },
      "source": [
        "yhat = mod.predict(X_test_reshaped, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jiw37MIctB8z"
      },
      "source": [
        "# calculate error in test set\n",
        "\n",
        "yhat_reshaped = yhat.reshape(y_test.shape[0], 340*12)\n",
        "y_test_reshaped=  y_test_reshaped.reshape(y_test_reshaped.shape[0], 340*12)\n",
        "testScore = mean_squared_error(yhat_reshaped, y_test_reshaped, squared=False)\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}